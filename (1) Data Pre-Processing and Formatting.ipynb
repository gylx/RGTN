{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beijing Multi-Site Air-Quality\n",
    "\n",
    "[data link](https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw data\n",
    "files = glob.glob('Raw Data/Beijing Multi-Site Air-Quality/*')\n",
    "data_dict = {file: pd.read_csv(file) for file in files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove useless variables\n",
    "for file in files:\n",
    "    data_dict[file].drop('No', axis=1, inplace=True)\n",
    "    data_dict[file].drop('station', axis=1, inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode date time\n",
    "for file in files:\n",
    "    time_cols = ['year', 'month', 'day', 'hour']\n",
    "    idx = pd.to_datetime(data_dict[file][time_cols])\n",
    "    data_dict[file].index = idx\n",
    "    data_dict[file].drop(time_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "for file in files:\n",
    "    data_dict[file] = pd.concat([\n",
    "        pd.get_dummies(data_dict[file]['wd'], prefix='wd'),\n",
    "        data_dict[file].drop('wd', axis=1),\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill nan with median\n",
    "for file in files:\n",
    "    data_dict[file] = data_dict[file].fillna(data_dict[file].median(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max Scaling\n",
    "for file in files:\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data_dict[file] = pd.DataFrame(\n",
    "        scaler.fit_transform(data_dict[file]),\n",
    "        index = data_dict[file].index,\n",
    "        columns = data_dict[file].columns,        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all columns align\n",
    "assert(all([all(data_dict[files[0]].columns==data_dict[file].columns) for file in files]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all indices align\n",
    "assert(all([all(data_dict[files[0]].index==data_dict[file].index) for file in files]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create X and Y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "lbw = 6  # look-back window\n",
    "skip = 2  # skip time-steps\n",
    "y_var = 'PM2.5'  # prediction variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35064, 12, 27)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creat data tensor\n",
    "data = np.array([data_dict[file].values for file in files])\n",
    "data = np.transpose(data, [1, 0, 2])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17529, 6, 12, 27) (17529, 12)\n"
     ]
    }
   ],
   "source": [
    "# Generate X and Y from data tensor\n",
    "y_var_idx = np.argmax(data_dict[file].columns == y_var)\n",
    "iter_range = range(lbw, data.shape[0], skip)\n",
    "X = np.array([data[i-lbw:i] for i in iter_range])\n",
    "Y = np.array([data[i, :, y_var_idx] for i in iter_range])\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "with open('ML DATA/air_quality.pkl', 'wb') as file:\n",
    "    data = (X, Y)\n",
    "    pickle.dump(data, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate Change\n",
    "\n",
    "[data link](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data = pd.read_csv('Raw Data/Climate Change/GlobalLandTemperaturesByMajorCity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index to datetime\n",
    "data['dt'] = pd.to_datetime(data['dt'], yearfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by country\n",
    "data = data[data['Country'] == 'India']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Features\n",
    "data['UpperTemp'] = data['AverageTemperature'] + data['AverageTemperatureUncertainty']\n",
    "data['LowerTemp'] = data['AverageTemperature'] - data['AverageTemperatureUncertainty']\n",
    "data['month'] = -np.cos(data['dt'].dt.month.values*2*np.pi/12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate tensor slices\n",
    "t_slice = data.pivot('dt', 'City', 'AverageTemperature').sort_index().resample('1M').first().ffill().bfill()\n",
    "ut_slice = data.pivot('dt', 'City', 'UpperTemp').sort_index().resample('1M').first().ffill().bfill()\n",
    "lt_slice = data.pivot('dt', 'City', 'LowerTemp').sort_index().resample('1M').first().ffill().bfill()\n",
    "m_slice = data.pivot('dt', 'City', 'month').sort_index().resample('1M').first().ffill().bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(all(t_slice.index == ut_slice.index) & all(t_slice.columns == ut_slice.columns))\n",
    "assert(all(t_slice.index == lt_slice.index) & all(t_slice.columns == lt_slice.columns))\n",
    "assert(all(t_slice.index == m_slice.index) & all(t_slice.columns == m_slice.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2613, 14, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate final tensor\n",
    "scaler = StandardScaler()\n",
    "data = np.array([\n",
    "    scaler.fit_transform(t_slice.values), \n",
    "    scaler.fit_transform(ut_slice.values), \n",
    "    scaler.fit_transform(lt_slice.values), \n",
    "    scaler.fit_transform(m_slice.values),\n",
    "])\n",
    "data = np.transpose(data, [1, 2, 0])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2607, 6, 14, 4) (2607, 14)\n"
     ]
    }
   ],
   "source": [
    "# Generate X and Y from data tensor\n",
    "lbw = 6  # look-back window\n",
    "skip = 1  # skip time-steps\n",
    "y_var_idx = 0  # prediction variable\n",
    "iter_range = range(lbw, data.shape[0], skip)\n",
    "X = np.array([data[i-lbw:i] for i in iter_range])\n",
    "Y = np.array([data[i, :, y_var_idx] for i in iter_range])\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "with open('ML DATA/climate_change.pkl', 'wb') as file:\n",
    "    data = (X, Y)\n",
    "    pickle.dump(data, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UK House Price Index\n",
    "\n",
    "[data link](http://publicdata.landregistry.gov.uk/market-trend-data/house-price-index-data/Average-prices-Property-Type-2020-10.csv?utm_medium=GOV.UK&utm_source=datadownload&utm_campaign=average_price_property_price&utm_term=9.30_16_12_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "data_url = 'http://publicdata.landregistry.gov.uk/market-trend-data/house-price-index-data/Average-prices-Property-Type-2020-10.csv?utm_medium=GOV.UK&utm_source=datadownload&utm_campaign=average_price_property_price&utm_term=9.30_16_12_20'\n",
    "data = pd.read_csv(data_url)\n",
    "data.set_index('Date', inplace=True)\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by London\n",
    "data = data[data['Region_Name'] == 'Liverpool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove useless variables\n",
    "data = data.drop(['Region_Name', 'Area_Code'], axis=1)\n",
    "data = data[[c for c in data.columns if ('Price' not in c) and ('Index' not in c)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fillna\n",
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "data = pd.DataFrame(scaler.fit_transform(data), index=data.index, columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310, 2)\n",
      "(310, 2)\n",
      "(310, 2)\n",
      "(310, 2)\n",
      "(310, 4, 2)\n"
     ]
    }
   ],
   "source": [
    "# Formulate the tensor\n",
    "data_tensor = []\n",
    "property_types = ['Detached', 'Semi', 'Terraced', 'Flat']\n",
    "for pt in property_types:\n",
    "    \n",
    "    data_slice = data[[c for c in data.columns if pt in c.split('_')[0]]]\n",
    "    print(data_slice.shape)\n",
    "    data_tensor.append(data_slice.values)\n",
    "    \n",
    "data = np.array(data_tensor)\n",
    "data = np.transpose(data, [1, 0, 2])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(304, 6, 4, 2) (304, 4)\n"
     ]
    }
   ],
   "source": [
    "# Generate X and Y from data tensor\n",
    "lbw = 6  # look-back window\n",
    "skip = 1  # skip time-steps\n",
    "y_var_idx = 0  # prediction variable\n",
    "iter_range = range(lbw, data.shape[0], skip)\n",
    "X = np.array([data[i-lbw:i] for i in iter_range])\n",
    "Y = np.array([data[i, :, y_var_idx] for i in iter_range])\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "with open('ML DATA/house_price.pkl', 'wb') as file:\n",
    "    data = (X, Y)\n",
    "    pickle.dump(data, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity Recognition\n",
    "\n",
    "[data link](http://archive.ics.uci.edu/ml/datasets/Activity+Recognition+system+based+on+Multisensor+data+fusion+%28AReM%29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bending1 7 (480, 6)\n",
      "bending1 6 (480, 6)\n",
      "bending1 4 (480, 6)\n",
      "bending1 5 (480, 6)\n",
      "bending1 1 (480, 6)\n",
      "bending1 2 (480, 6)\n",
      "bending1 3 (480, 6)\n",
      "walking 7 (480, 6)\n",
      "walking 6 (480, 6)\n",
      "walking 4 (480, 6)\n",
      "walking 5 (480, 6)\n",
      "walking 1 (480, 6)\n",
      "walking 2 (480, 6)\n",
      "walking 3 (480, 6)\n",
      "walking 10 (480, 6)\n",
      "walking 11 (480, 6)\n",
      "walking 13 (480, 6)\n",
      "walking 12 (480, 6)\n",
      "walking 15 (480, 6)\n",
      "walking 14 (480, 6)\n",
      "walking 8 (480, 6)\n",
      "walking 9 (480, 6)\n",
      "bending2 6 (480, 6)\n",
      "bending2 5 (480, 6)\n",
      "bending2 1 (480, 6)\n",
      "bending2 2 (480, 6)\n",
      "bending2 3 (480, 6)\n",
      "standing 7 (480, 6)\n",
      "standing 6 (480, 6)\n",
      "standing 4 (480, 6)\n",
      "standing 5 (480, 6)\n",
      "standing 1 (480, 6)\n",
      "standing 2 (480, 6)\n",
      "standing 3 (480, 6)\n",
      "standing 10 (480, 6)\n",
      "standing 11 (480, 6)\n",
      "standing 13 (480, 6)\n",
      "standing 12 (480, 6)\n",
      "standing 15 (480, 6)\n",
      "standing 14 (480, 6)\n",
      "standing 8 (480, 6)\n",
      "standing 9 (480, 6)\n",
      "sitting 7 (480, 6)\n",
      "sitting 6 (480, 6)\n",
      "sitting 4 (480, 6)\n",
      "sitting 5 (480, 6)\n",
      "sitting 1 (480, 6)\n",
      "sitting 2 (480, 6)\n",
      "sitting 3 (480, 6)\n",
      "sitting 10 (480, 6)\n",
      "sitting 11 (480, 6)\n",
      "sitting 13 (480, 6)\n",
      "sitting 12 (480, 6)\n",
      "sitting 15 (480, 6)\n",
      "sitting 14 (480, 6)\n",
      "sitting 8 (479, 6)\n",
      "sitting 9 (480, 6)\n",
      "lying 7 (480, 6)\n",
      "lying 6 (480, 6)\n",
      "lying 4 (480, 6)\n",
      "lying 5 (480, 6)\n",
      "lying 1 (480, 6)\n",
      "lying 2 (480, 6)\n",
      "lying 3 (480, 6)\n",
      "lying 10 (480, 6)\n",
      "lying 11 (480, 6)\n",
      "lying 13 (480, 6)\n",
      "lying 12 (480, 6)\n",
      "lying 15 (480, 6)\n",
      "lying 14 (480, 6)\n",
      "lying 8 (480, 6)\n",
      "lying 9 (480, 6)\n",
      "cycling 7 (480, 6)\n",
      "cycling 6 (480, 6)\n",
      "cycling 4 (480, 6)\n",
      "cycling 5 (480, 6)\n",
      "cycling 1 (480, 6)\n",
      "cycling 2 (480, 6)\n",
      "cycling 3 (480, 6)\n",
      "cycling 10 (480, 6)\n",
      "cycling 11 (480, 6)\n",
      "cycling 13 (480, 6)\n",
      "cycling 12 (480, 6)\n",
      "cycling 15 (480, 6)\n",
      "cycling 14 (480, 6)\n",
      "cycling 8 (480, 6)\n",
      "cycling 9 (480, 6)\n"
     ]
    }
   ],
   "source": [
    "# Read raw data\n",
    "data_dict = {}\n",
    "folders = glob.glob('Raw Data/Activity Recognition Multisensor/*')\n",
    "for folder in folders:\n",
    "    \n",
    "    category = folder.split('/')[-1]\n",
    "    data_dict[category] = {}\n",
    "    \n",
    "    category_files = glob.glob(f'{folder}/*')\n",
    "    for file in category_files:\n",
    "        file_df = pd.read_csv(file, header=4, index_col=0)\n",
    "        file_df.index = file_df.index.rename('time')\n",
    "        data_n = file.split('/')[-1].replace('.csv', '').replace('dataset', '')\n",
    "        data_dict[category][data_n] = file_df\n",
    "        print(category, data_n, file_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((34199, 24, 3, 3), (34199, 5))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format data tensor\n",
    "lbw = 24\n",
    "scaler = StandardScaler()\n",
    "X, Y = [], []\n",
    "for category in data_dict.keys():\n",
    "    \n",
    "    if 'bending' not in category:\n",
    "    \n",
    "        df_tensors = []\n",
    "        for data_n in data_dict[category].keys():\n",
    "\n",
    "            df = data_dict[category][data_n]\n",
    "            df = df.fillna(df.mean())\n",
    "\n",
    "            df1 = df[['avg_rss12']].copy()\n",
    "            df1['upper_avg_rss12'] = df['avg_rss12'] + df['var_rss12']\n",
    "            df1['lower_avg_rss12'] = df['avg_rss12'] - df['var_rss12']\n",
    "\n",
    "            df2 = df[['avg_rss13']].copy()\n",
    "            df2['upper_avg_rss13'] = df['avg_rss13'] + df['var_rss13']\n",
    "            df2['lower_avg_rss13'] = df['avg_rss13'] - df['var_rss13']\n",
    "\n",
    "            df3 = df[['avg_rss23']].copy()\n",
    "            df3['upper_avg_rss23'] = df['avg_rss23'] + df['var_rss23']\n",
    "            df3['lower_avg_rss23'] = df['avg_rss23'] - df['var_rss23']\n",
    "\n",
    "            df_tensor = np.array([\n",
    "                scaler.fit_transform(df1),\n",
    "                scaler.fit_transform(df2),\n",
    "                scaler.fit_transform(df3),\n",
    "            ])\n",
    "\n",
    "            df_tensor = np.transpose(df_tensor, [1, 0, 2])  # T x S x F\n",
    "            df_X = np.array([df_tensor[i-lbw:i] for i in range(lbw, df_tensor.shape[0])])\n",
    "            df_Y = np.array([category, ] * df_X.shape[0])\n",
    "            X.append(df_X)\n",
    "            Y.append(df_Y)\n",
    "            \n",
    "X = np.concatenate(X)\n",
    "Y = np.concatenate(Y)\n",
    "Y = pd.get_dummies(Y).values\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "with open('ML DATA/activity_recognition.pkl', 'wb') as file:\n",
    "    data = (X, Y)\n",
    "    pickle.dump(data, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
